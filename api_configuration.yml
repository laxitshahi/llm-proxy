llm_proxy:
  name: LLM proxy API
  version: demo 1.0
  description: An API that act as a proxy to multiple language models

user_settings:
  # Example
  # - routing_mode: 
  #   - cost_routing:
  #       enable: false
    
  - model: OpenAI
    api_key_var: OPENAI_API_KEY # .env name for api key 
    max_output_tokens: 256 
    temperature: 0.1 
    models: # input names of models you want to you (see below for all models provided)
      # Only models specified here will be added to routing pool
      - gpt-3.5-turbo-instruct
      - gpt-3.5-turbo-1106
      - gpt-4
      - gpt-4-32k
    # context: "Answer politely"

  - model: Llama2
    temperature: 0.1
    api_key_var: LLAMA2_API_KEY
    max_output_tokens: 256
    models:
      - Llama-2-7b-chat-hf
      - Llama-2-7b-chat
      - Llama-2-7b-hf
      - Llama-2-7b
      - Llama-2-13b-chat-hf
      - Llama-2-13b-chat
      - Llama-2-13b-hf
      - Llama-2-13b
      - Llama-2-70b-chat-hf
      - Llama-2-70b-chat
      - Llama-2-70b-hf
      - Llama-2-70b

  - model: Cohere 
    temperature: 0.1
    api_key_var: COHERE_API_KEY 
    max_output_tokens: 256
    models:
      - command
      - command-light
      - command-nightly
      - command-light-nightly

  - model: Mistral
    temperature: 0.1
    api_key_var: MISTRAL_API_KEY
    max_output_tokens: 256
    models:
      - Mistral-7B-v0.1
      - Mistral-7B-Instruct-v0.2
      - Mistral-8x7B-Instruct-v0.1

  - model: VertexAI 
    temperature: 0.1
    project_id_var: GOOGLE_PROJECT_ID 
    max_output_tokens: 256 
    models:
      - text-bison
      # Currently not working
      # - chat-bison

available_models:
  - name: OpenAI
    class: llmproxy.models.openai.OpenAI
    models:
      - name: gpt-3.5-turbo-1106
        cost_per_token_input: 0.0000010
        cost_per_token_output: 0.0000020
      - name: gpt-3.5-turbo-instruct
        cost_per_token_input: 0.0000015
        cost_per_token_output: 0.0000020
      - name: gpt-4
        cost_per_token_input: 0.00003
        cost_per_token_output: 0.00006
      - name: gpt-4-32k
        cost_per_token_input: 0.00006
        cost_per_token_output: 0.00012

  - name: Llama2
    class: llmproxy.models.llama2.Llama2
    models:
      - name: Llama-2-7b-chat-hf
        cost_per_token_input: 0.00000005
        cost_per_token_output: 0.00000025

      - name: Llama-2-7b-chat
        cost_per_token_input: 0.00000005
        cost_per_token_output: 0.00000025

      - name: Llama-2-7b-hf
        cost_per_token_input: 0.00000005
        cost_per_token_output: 0.00000025

      - name: Llama-2-7b
        cost_per_token_input: 0.00000005
        cost_per_token_output: 0.00000025

      - name: Llama-2-13b-chat-hf
        cost_per_token_input: 0.0000001
        cost_per_token_output: 0.0000005

      - name: Llama-2-13b-chat
        cost_per_token_input: 0.0000001
        cost_per_token_output: 0.0000005

      - name: Llama-2-13b-hf
        cost_per_token_input: 0.0000001
        cost_per_token_output: 0.0000005

      - name: Llama-2-13b
        cost_per_token_input: 0.0000001
        cost_per_token_output: 0.0000005

      - name: Llama-2-70b-chat-hf
        cost_per_token_input: 0.00000065
        cost_per_token_output: 0.00000275

      - name: Llama-2-70b-chat
        cost_per_token_input: 0.00000065
        cost_per_token_output: 0.00000275

      - name: Llama-2-70b-hf
        cost_per_token_input: 0.00000065
        cost_per_token_output: 0.00000275

      - name: Llama-2-70b
        cost_per_token_input: 0.00000065
        cost_per_token_output: 0.00000275

  - name: Mistral
    class: llmproxy.models.mistral.Mistral
    models:
      # [num]/1,000,000
      - name: Mistral-7B-v0.1
        cost_per_token_input: 0.00000005
        cost_per_token_output: 0.00000025

      # [num]/1,000,000
      - name: Mistral-7B-Instruct-v0.2
        cost_per_token_input: 0.00000005
        cost_per_token_output: 0.00000025

      # [num]/1,000,000
      - name: Mistral-8x7B-Instruct-v0.1
        cost_per_token_input: 0.0000003
        cost_per_token_output: 0.000001

  - name: Cohere
    class: llmproxy.models.cohere.Cohere
    models:
      - name: command
        cost_per_token_input: 0.0000015
        cost_per_token_output: 0.000002
      - name: command-light
        cost_per_token_input: 0.0000015
        cost_per_token_output: 0.000002
      - name: command-nightly
        cost_per_token_input: 0.0000015
        cost_per_token_output: 0.000002
      - name: command-light-nightly
        cost_per_token_input: 0.0000015
        cost_per_token_output: 0.000002

  - name: Vertexai
    class: llmproxy.models.vertexai.VertexAI
    models:
      - name: text-bison
        cost_per_token_input: 0.0000015
        cost_per_token_output: 0.000002
      # Currently not working
      # - name: chat-bison-32k
      #   cost_per_token_input: 0.000002
      #   cost_per_token_output: 0.000002
